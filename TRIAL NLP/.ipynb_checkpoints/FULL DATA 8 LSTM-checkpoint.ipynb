{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFuL-RBgXqgU"
   },
   "source": [
    "## Teks Summarisation Using Deep Learning for Analyze Customer Review of Amazon Fine Food Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJDvJ7k4DqNa"
   },
   "source": [
    "### Kelompok\n",
    "Anggota 1 : Muhammad Hussein 201710370311191\n",
    "<br>\n",
    "Anggota 2 : Moch. Chamdani Mustaqim 201710370311285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpWts88MRPUv"
   },
   "source": [
    "# **Permasalahan Yang Kami Angkat**\n",
    "\n",
    "Ulasan dari pelanggan seringkali panjang dan deskriptif. Dengan menganalisis ulasan ini secara manual benar-benar memakan waktu. Di sinilah kelebihan **Pemrosesan Bahasa Alami** dapat diterapkan untuk menghasilkan ringkasan untuk ulasan yang panjang.\n",
    "\n",
    "Kami di sini menggunakan dataset ulasan pelanggan Amazon Fine Food. Tujuan kami adalah untuk menghasilkan ringkasan ulasan pelanggan Amazon Fine Food menggunakan pendekatan berbasis deep learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "# **Custom Attention Layer**\n",
    "\n",
    "Keras tidak secara resmi mendukung Attention Layer. Jadi, kami mengimplementasikan Attention Layer dari github public. Kami mengunduh Attention Layer dari [sini](https://github.com/thushv89/attention_keras/blob/master/layers/attention.py) kemudian menyalin dan menyimpannya di file berbeda bernama attention.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIujPSa_TwkS"
   },
   "source": [
    "# Mount Google Drive dan Upload file attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9cUYuclPTpC"
   },
   "outputs": [],
   "source": [
    "import attention \n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUValOzcHtEK"
   },
   "source": [
    "# Impor Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2935,
     "status": "ok",
     "timestamp": 1587499127648,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "_Jpu8qLEFxcY",
    "outputId": "c7205673-0e83-47da-f8e9-e9d2e53d68cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVakjZ3oICgx"
   },
   "source": [
    "# Read dataset\n",
    "\n",
    "Dataset ini terdiri dari ulasan makanan enak dari Amazon. Data ini mencakup periode lebih dari 10 tahun, termasuk semua ~ 500.000 ulasan hingga Oktober 2012. Ulasan ini mencakup produk dan informasi pengguna, peringkat, ulasan teks biasa, dan ringkasan. Ini juga termasuk ulasan dari semua kategori Amazon lainnya.\n",
    "\n",
    "Kami akan mengambil sampel 100.000 ulasan untuk mengurangi waktu pelatihan model kami. Jangan ragu untuk menggunakan seluruh dataset untuk melatih model Anda jika mesin Anda memiliki kekuatan komputasi semacam itu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnK5o4Z1Fxcj"
   },
   "outputs": [],
   "source": [
    "# data=pd.read_csv(path+\"Reviews.csv\",nrows=100000)\n",
    "data=pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGNQKvCaISIn"
   },
   "source": [
    "# Drop Duplicates dan NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cjul88oOFxcr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393565, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qi0xD6BkIWAm"
   },
   "source": [
    "# Informasi tentang dataset\n",
    "\n",
    "Mari kita lihat tipe data dan bentuk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1587499138852,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "__fy-JxTFxc9",
    "outputId": "b201dde8-875d-431d-b514-c75cf475b4fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 393565 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      "Id                        393565 non-null int64\n",
      "ProductId                 393565 non-null object\n",
      "UserId                    393565 non-null object\n",
      "ProfileName               393565 non-null object\n",
      "HelpfulnessNumerator      393565 non-null int64\n",
      "HelpfulnessDenominator    393565 non-null int64\n",
      "Score                     393565 non-null int64\n",
      "Time                      393565 non-null int64\n",
      "Summary                   393565 non-null object\n",
      "Text                      393565 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 33.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0xLYACiFxdJ"
   },
   "source": [
    "#Preprocessing\n",
    "\n",
    "Melakukan langkah-langkah Preprocessing dasar sangat penting sebelum kami sampai pada bagian pembuatan model. Menggunakan data teks yang tidak beraturan dan tidak bersih adalah langkah yang berpotensi menimbulkan bencana dalam pemrosesan bahasa alami. Jadi pada langkah ini, kami akan membuang semua simbol, karakter, dll yang tidak diinginkan dari teks yang tidak mempengaruhi tujuan masalah yang kami angkat.\n",
    "\n",
    "Berikut ini adalah kamus yang akan kami gunakan untuk memperluas kontraksi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s6IY-x2FxdL"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rvWj-_lVq7q"
   },
   "source": [
    "### **Download stopwords dari library nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1312,
     "status": "ok",
     "timestamp": 1587499147978,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "qwi-CnyDKVxB",
    "outputId": "97369fa1-3ac2-48b7-df97-5b3f709e4bfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JFRXFHmI7Mj"
   },
   "source": [
    "Kami akan melakukan sedikit pemrosesan awal untuk data kami:\n",
    "\n",
    "1. Konversikan semuanya menjadi huruf kecil\n",
    "\n",
    "2. Hapus tag HTML\n",
    "\n",
    "3. Pemetaan kontraksi\n",
    "\n",
    "4. Hapus (â€˜s)\n",
    "\n",
    "5. Hapus teks apa pun di dalam tanda kurung ()\n",
    "\n",
    "6. Hilangkan tanda baca dan karakter khusus\n",
    "\n",
    "7. Hapus stopwords\n",
    "\n",
    "8. Hapus kata-kata pendek\n",
    "\n",
    "berikut fungsi yang kami gunakan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZr-u3OEFxdT"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 # menghapus kata - kata pendek\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2QAeCHWFxdY"
   },
   "outputs": [],
   "source": [
    "#memanggil fungsi\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t,0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snRZY8wjLao2"
   },
   "source": [
    "### **Melihat ulasan 5 pelanggan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110780,
     "status": "ok",
     "timestamp": 1587499264957,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "NCAIkhWbFxdh",
    "outputId": "cd6500d4-82cd-4e2e-9c82-df8e6f83ed92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsRXocxoFxd-"
   },
   "outputs": [],
   "source": [
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZeD0gs6Lnb-"
   },
   "source": [
    "### **Melihat 10 ringkasan pertama yang telah dilakukan preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 176209,
     "status": "ok",
     "timestamp": 1587499342307,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "jQJdZcAzFxee",
    "outputId": "5c05be75-05b7-45dc-801b-2bc04ea8c789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not as advertised',\n",
       " 'delight says it all',\n",
       " 'cough medicine',\n",
       " 'great taffy',\n",
       " 'nice taffy',\n",
       " 'great just as good as the expensive brands',\n",
       " 'wonderful tasty taffy',\n",
       " 'yay barley',\n",
       " 'healthy dog food',\n",
       " 'the best hot sauce in the world',\n",
       " 'my cats love this diet food better than their regular food',\n",
       " 'my cats are not fans of the new food',\n",
       " 'fresh and greasy',\n",
       " 'strawberry twizzlers yummy',\n",
       " 'lots of twizzlers just what you expect',\n",
       " 'poor taste',\n",
       " 'love it',\n",
       " 'great sweet candy',\n",
       " 'home delivered twizlers',\n",
       " 'always fresh',\n",
       " 'twizzlers',\n",
       " 'delicious product',\n",
       " 'twizzlers',\n",
       " 'please sell these in mexico',\n",
       " 'twizzlers strawberry',\n",
       " 'nasty no flavor',\n",
       " 'great bargain for the price',\n",
       " 'yummy',\n",
       " 'great machine',\n",
       " 'this is my taste',\n",
       " 'best of the instant oatmeals',\n",
       " 'good instant',\n",
       " 'great irish oatmeal for those in hurry',\n",
       " 'satisfying',\n",
       " 'love gluten free oatmeal',\n",
       " 'it is oatmeal',\n",
       " 'good way to start the day',\n",
       " 'wife favorite breakfast',\n",
       " 'why would not you buy oatmeal from mcanns tastes great',\n",
       " 'oatmeal for oatmeal lovers',\n",
       " 'food great',\n",
       " 'good hot breakfast',\n",
       " 'great taste and convenience',\n",
       " 'hearty oatmeal',\n",
       " 'good',\n",
       " 'mushy',\n",
       " 'very good but next time will not order the variety pack',\n",
       " 'same stuff',\n",
       " 'do not like it',\n",
       " 'hot and good came back for more',\n",
       " 'you will go nuts over ass kickin peanuts',\n",
       " 'not ass kickin',\n",
       " 'roasts up smooth brew',\n",
       " 'our guests love it',\n",
       " 'awesome deal',\n",
       " 'how can you go wrong',\n",
       " 'awsome kids in neighborhood loved us',\n",
       " 'great deal',\n",
       " 'better price for this at target',\n",
       " 'pretty expensive',\n",
       " 'stale product',\n",
       " 'hammer nutrition fizz rocks',\n",
       " 'great source of electrolytes',\n",
       " 'great for preventing cramps',\n",
       " 'low carb alternative to gatorade',\n",
       " 'taste is not so good',\n",
       " 'how much would you pay for bag of chocolate pretzels',\n",
       " 'pretzel haven',\n",
       " 'great gummi',\n",
       " 'bigger then other brands',\n",
       " 'best ever latice tart',\n",
       " 'warning warning alcohol sugars',\n",
       " 'nothing special',\n",
       " 'no tea flavor',\n",
       " 'good',\n",
       " 'taste great',\n",
       " 'order only in cold weather',\n",
       " 'this is the best',\n",
       " 'delicious',\n",
       " 'great',\n",
       " 'forget molecular gastronomy this stuff rockes coffee creamer',\n",
       " 'natural balance lamb and rice',\n",
       " 'increased my dogs itching',\n",
       " 'great food',\n",
       " 'great for my dogs allergies',\n",
       " 'great for stomach problems',\n",
       " 'better life for you dog',\n",
       " 'great food',\n",
       " 'great food for my my dog who has sensitive stomach',\n",
       " 'great dog food',\n",
       " 'mm mm good',\n",
       " 'great dog food',\n",
       " 'so convenient',\n",
       " 'good healthy dog food',\n",
       " 'great dog food',\n",
       " 'great allergy sensitive dog food dogs love it',\n",
       " 'perfect for our english bulldog with allergies',\n",
       " 'bad',\n",
       " 'taste wise it is star item',\n",
       " 'great support',\n",
       " 'tart',\n",
       " 'omaha apple tartlets',\n",
       " 'loved these tartlets',\n",
       " 'the best',\n",
       " 'disappointing',\n",
       " 'wasting vinegar on cucumber is shame',\n",
       " 'asparagus bliss',\n",
       " 'my idea of good diet food',\n",
       " 'low carb angel food puffs',\n",
       " 'delicious tea',\n",
       " 'my every day green tea',\n",
       " 'the best tea ever freah bright clean',\n",
       " 'tea review',\n",
       " 'wonderful tea',\n",
       " 'great cookies',\n",
       " 'best everyday cookie',\n",
       " 'so far so good',\n",
       " 'best cat food',\n",
       " 'great food',\n",
       " 'perfect cat food for older cats',\n",
       " 'good for feline uti',\n",
       " 'palatable and healthy',\n",
       " 'healthy they love it',\n",
       " 'wonderful food perfect for allergic kitties',\n",
       " 'holistic select cat food',\n",
       " 'tastes great love hot spicy bad price here',\n",
       " 'my favorite ramen',\n",
       " 'it burns',\n",
       " 'amazing to the last bite',\n",
       " 'not for me',\n",
       " 'great spicy flavor',\n",
       " 'great value and convenient ramen',\n",
       " 'great flavor',\n",
       " 'tastes great but is cheaper locally',\n",
       " 'tastes awesome looks beautiful',\n",
       " 'happy face',\n",
       " 'simply the best',\n",
       " 'excellent product life saver',\n",
       " 'nice snack',\n",
       " 'good licorice',\n",
       " 'love these',\n",
       " 'great for the kids',\n",
       " 'bite sized',\n",
       " 'sweet with nice kick',\n",
       " 'broken bottle bottoms',\n",
       " 'love the salsa',\n",
       " 'ehhh',\n",
       " 'awesome cornmeal',\n",
       " 'great marinade',\n",
       " 'awesome stuff',\n",
       " 'tastes good',\n",
       " 'rip off price',\n",
       " 'jell',\n",
       " 'great flavor of jell',\n",
       " 'great deal',\n",
       " 'great tasting sea salt with iodine',\n",
       " 'tastes very fresh',\n",
       " 'simple but good',\n",
       " 'tasty',\n",
       " 'not the greatest tasting',\n",
       " 'not bad',\n",
       " 'right size taste',\n",
       " 'tasteless but low calorie',\n",
       " 'this stuff is sooooo good',\n",
       " 'best stuff ever',\n",
       " 'very low quality',\n",
       " 'not banana runts',\n",
       " 'banana heads not banana runts',\n",
       " 'worked great',\n",
       " 'ricore forever',\n",
       " 'delicious',\n",
       " 'fluffy soft delicious and sugary sweet',\n",
       " 'great but not as good as it was back in the day as teen',\n",
       " 'excellent lemon juice',\n",
       " 'great product',\n",
       " 'handy',\n",
       " 'realemon juice from amazon',\n",
       " 'relaxing almost like something you smoke',\n",
       " 'never paid that much',\n",
       " 'marley mellow mood lite half tea half lemonade',\n",
       " 'great product to help you sleep',\n",
       " 'perfect for gluten free chocolate chip cookies',\n",
       " 'garbonzo bean flour',\n",
       " 'yum falafel',\n",
       " 'make fresh fruit tart light and beautiful',\n",
       " 'miracle',\n",
       " 'not bad for instant healthy coffee',\n",
       " 'it is ok',\n",
       " 'love it',\n",
       " 'great taste and has health benefits',\n",
       " 'tastes great arrived in days',\n",
       " 'great for after lunch',\n",
       " 'nice little mints but pricey',\n",
       " 'altoids mini mints tins',\n",
       " 'altoids smalls wintergreen',\n",
       " 'sugarfree',\n",
       " 'tasty',\n",
       " 'these mints are awesome',\n",
       " 'altoids smalls',\n",
       " 'love these and reusable containers',\n",
       " 'altoids',\n",
       " 'huge hit at the office',\n",
       " 'love em they are great',\n",
       " 'love these',\n",
       " 'fresh',\n",
       " 'wintergreen me',\n",
       " 'these just do not do it for me as breath mints',\n",
       " 'better than average more expensive than average',\n",
       " 'my cat loves it',\n",
       " 'great for fat cats and senior citizens',\n",
       " 'nearly killed the cats',\n",
       " 'changed formula makes cats sick',\n",
       " 'best by the case',\n",
       " 'looking for different flavor',\n",
       " 'price cannot be correct',\n",
       " 'more hot spicy than mccormick brand',\n",
       " 'ahmad loose imperial blend tea is great for the price',\n",
       " 'nice tea',\n",
       " 'fragrant tea',\n",
       " 'best ahmad tea',\n",
       " 'my favorite tea',\n",
       " 'best tea ever',\n",
       " 'not real tea',\n",
       " 'delicious',\n",
       " 'best bloody mary mixer',\n",
       " 'the best',\n",
       " 'mcclures bloody mary mix',\n",
       " 'not good',\n",
       " 'love this tea',\n",
       " 'really nice taste',\n",
       " 'just give me some watermelon and citron sea salt',\n",
       " 'furniture polish taste',\n",
       " 'big tub salt',\n",
       " 'taste is neutral quantity is deceitful',\n",
       " 'eukanuba puppy small breed dog food',\n",
       " 'high quality but it gave my dog wicked gas',\n",
       " 'great tasting green tea and such great deal',\n",
       " 'omg best chocolate jelly belly',\n",
       " 'excellent loose tea',\n",
       " 'good anytime hot tea',\n",
       " 'my everyday cup of tea',\n",
       " 'this is what you get in the store',\n",
       " 'ahmad tea',\n",
       " 'disappointed',\n",
       " 'wonderful',\n",
       " 'best way to buy kcups',\n",
       " 'delicious',\n",
       " 'keeps you out of the dentest chair',\n",
       " 'super superfoods are super easy',\n",
       " 'pok chops',\n",
       " 'sad outcome',\n",
       " 'best energy shot for me',\n",
       " 'do not waste your money',\n",
       " 'if you cannot handle caffeine this is not for you',\n",
       " 'yum yummy yummier',\n",
       " 'reeks like chemicals',\n",
       " 'disappointed',\n",
       " 'great for gluten free lifestyle',\n",
       " 'my dog loves these',\n",
       " 'yummy',\n",
       " 'sugar in the raw',\n",
       " 'manufacturing problems diminish product',\n",
       " 'good product but terrible agricultural practices',\n",
       " 'sugar in the raw',\n",
       " 'sugar in the raw',\n",
       " 'lie',\n",
       " 'it is sugar',\n",
       " 'excellent but not perfect',\n",
       " 'good product',\n",
       " 'you will never use white sugar again',\n",
       " 'thanks for the review scott',\n",
       " 'great',\n",
       " 'awesome sugar',\n",
       " 'great product weak packaging',\n",
       " 'excellent',\n",
       " 'excellent for',\n",
       " 'amazing',\n",
       " 'very tasty chips',\n",
       " 'yummy',\n",
       " 'excellent taste',\n",
       " 'over priced chips and lack rice taste',\n",
       " 'rotel saves me on daily basis',\n",
       " 'it is fabulous but not from amazon',\n",
       " 'too expensive',\n",
       " 'not mild enough for me lol',\n",
       " 'great natural energy',\n",
       " 'like this stuff',\n",
       " 'great energy',\n",
       " 'not sure',\n",
       " 'the best energy shot out there smooth and organic',\n",
       " 'fantastic natural energy',\n",
       " 'way better than guayaki',\n",
       " 'does not taste that good but provides you the energy',\n",
       " 'rocket in bottle',\n",
       " 'this stuff works',\n",
       " 'favorite energy shot and all natural too',\n",
       " 'natural energy boost',\n",
       " 'best energy shot have ever tasted',\n",
       " 'the best',\n",
       " 'tested by trucker',\n",
       " 'good stuff',\n",
       " 'great energy drink without artificial ingredients',\n",
       " 'flavor getting better energy is great',\n",
       " 'so awful can barely describe',\n",
       " 'could use only once',\n",
       " 'vanilla tootsie rolls',\n",
       " 'one of my favoritte foods',\n",
       " 'fantastic',\n",
       " 'wow',\n",
       " 'very dissapointed',\n",
       " 'what quantity is it',\n",
       " 'very good coffee',\n",
       " 'very tasty',\n",
       " 'excellent coffee',\n",
       " 'hot',\n",
       " 'hot and delicious',\n",
       " 'swiss chalet',\n",
       " 'oyster sauce',\n",
       " 'absolutely delicious',\n",
       " 'great gag gift',\n",
       " 'arrived fast',\n",
       " 'penguin pooper',\n",
       " 'never arrived',\n",
       " 'coffee mate coffee creamer hazelnut',\n",
       " 'no broken creamers',\n",
       " 'shipped great',\n",
       " 'better packaging',\n",
       " 'coffee mate creamer',\n",
       " 'perfect for work',\n",
       " 'awful',\n",
       " 'pop tarts work of art',\n",
       " 'yes this is real excellent coffee',\n",
       " 'does not taste very good',\n",
       " 'no no',\n",
       " 'the king of all seasoning salts',\n",
       " 'toasted sesame oil',\n",
       " 'tasty tasty tasty',\n",
       " 'love love love these',\n",
       " 'great for eating whole foods clean with veggie brush',\n",
       " 'absolutely love it',\n",
       " 'only good for ice',\n",
       " 'great for teething',\n",
       " 'wonderful idea difficult to clean',\n",
       " 'was not that impressed',\n",
       " 'love the fresh food feeder',\n",
       " 'great beans',\n",
       " 'good stuff',\n",
       " 'excellent exactly what expected',\n",
       " 'these are the best',\n",
       " 'love love love',\n",
       " 'the product is great but the price is out of line',\n",
       " 'swedish pearl is not the same as belgian pearl',\n",
       " 'perfect',\n",
       " 'great taste',\n",
       " 'taste tested by wine maker',\n",
       " 'excellent everyday olive oil',\n",
       " 'love weavers am fan',\n",
       " 'make my day',\n",
       " 'treat yourself to the best coffee',\n",
       " 'bitter',\n",
       " 'drinking it now love the latin america aroma',\n",
       " 'great snack',\n",
       " 'best bar',\n",
       " 'cannot find anywhere else',\n",
       " 'my new granola bar',\n",
       " 'another husband favorite',\n",
       " 'price surprise',\n",
       " 'very smooth coffee highly recommended',\n",
       " 'saving grace for green mountain coffee',\n",
       " 'my favorite',\n",
       " 'good coffee',\n",
       " 'nantucket blend cups',\n",
       " 'fantastic chicken noodle soup',\n",
       " 'greatest oil since slice bread',\n",
       " 'best ever',\n",
       " 'deliciously scrumptious',\n",
       " 'heinz no more',\n",
       " 'this is really good stuff',\n",
       " 'disappointing',\n",
       " 'waste of money',\n",
       " 'porcini mushrooms an excellent product',\n",
       " 'excellent flavor mostly large pieces',\n",
       " 'the best',\n",
       " 'good for the money',\n",
       " 'not the highest quality but good for the price',\n",
       " 'fresh whole perfect',\n",
       " 'fresh and tasty',\n",
       " 'valentine gift winner',\n",
       " 'cat will not go near it',\n",
       " 'simply wild chick brown rice for cats',\n",
       " 'ham base',\n",
       " 'msg ham base',\n",
       " 'delisious pancakes',\n",
       " 'great all around mix',\n",
       " 'great mix',\n",
       " 'perfect mix for egg allergic',\n",
       " 'arrowhead mills whole grain buttermilk pancakes are easy',\n",
       " 'good for egg allergy',\n",
       " 'love the product disappointed in the shipping',\n",
       " 'poor item packaging',\n",
       " 'awful',\n",
       " 'disappointing',\n",
       " 'great healthy snack',\n",
       " 'sweet and soothing',\n",
       " 'minty flavor',\n",
       " 'ingredients take about seconds to read',\n",
       " 'fantastic healthy product',\n",
       " 'great product',\n",
       " 'excellent tea',\n",
       " 'french roast bags',\n",
       " 'best roast ever',\n",
       " 'franch is the best',\n",
       " 'double the pleasure',\n",
       " 'very good great taste and easy for single guy',\n",
       " 'an acquired taste',\n",
       " 'look elsewhere for your whole grains',\n",
       " 'hey',\n",
       " 'these are famous for reason',\n",
       " 'wow',\n",
       " 'god love these cookies',\n",
       " 'hard',\n",
       " 'kettle potato chips fully loaded baked potato',\n",
       " 'fresh lightly spiced crunchy kettle chips good value good product',\n",
       " 'glad to find them in oz size',\n",
       " 'pretty good could be better',\n",
       " 'slight taste of jalapeno',\n",
       " 'best chips ever',\n",
       " 'kettle potato chips sweet onion',\n",
       " 'ridiculously good',\n",
       " 'delicious',\n",
       " 'pucker up',\n",
       " 'love these chips they are thick and crunchy',\n",
       " 'quite good',\n",
       " 'delicious',\n",
       " 'salty and vinegary',\n",
       " 'wow',\n",
       " 'best gluten free dairy free chips',\n",
       " 'unique flavor for fans of thai food',\n",
       " 'honey dijon leaves bad aftertaste ny cheddar are pretty good',\n",
       " 'yowzah',\n",
       " 'very good',\n",
       " 'excellent',\n",
       " 'tangy goodness',\n",
       " 'heavy on the vinegar',\n",
       " 'delicious crisp chip with good flavor',\n",
       " 'best buy in bbq chips',\n",
       " 'love them',\n",
       " 'crunchy and tasty',\n",
       " 'convenience at low cost',\n",
       " 'an acquired taste',\n",
       " 'best deal ever',\n",
       " 'yum',\n",
       " 'barbeque perfection',\n",
       " 'excellent thai flavored chip',\n",
       " 'best kettle chips',\n",
       " 'the supreme salt vinegar',\n",
       " 'delicious as always',\n",
       " 'oooh yummy',\n",
       " 'not quite the best',\n",
       " 'want to gaain twenty pounds with no control whatsoever buy this',\n",
       " 'one of their best flavors',\n",
       " 'love these chips',\n",
       " 'highly addicitive chips',\n",
       " 'these are awesome',\n",
       " 'one bite and you will become chippoisseur',\n",
       " 'crunchy salty sweet finally superbowl snack that scooores',\n",
       " 'these chips make me weak at the knees',\n",
       " 'by far my favorite chips',\n",
       " 'good chips more cheese',\n",
       " 'pretty good tasting chip',\n",
       " 'yummy chips',\n",
       " 'best sour cream onion chip have had',\n",
       " 'great chips',\n",
       " 'fabulous',\n",
       " 'box chips',\n",
       " 'fantastic you can not beat this taste and you can not resist to it only one if you are spice lover',\n",
       " 'an addictive potato chip',\n",
       " 'great chip',\n",
       " 'excellent balance of taste crunchiness and moisture',\n",
       " 'yum',\n",
       " 'very good chips at great price',\n",
       " 'of the chips in the bag are over cooked',\n",
       " 'great chips',\n",
       " 'not bad but little hard to get used to',\n",
       " 'firm quality chip',\n",
       " 'not very creamy or chivey',\n",
       " 'prefer other flavors',\n",
       " 'these will be habit forming',\n",
       " 'good and tangy',\n",
       " 'the best chips ever',\n",
       " 'do not even like kettle chips and love these',\n",
       " 'you have to love sea salt and vinegar already',\n",
       " 'addictive',\n",
       " 'amazing chips',\n",
       " 'best chip ever',\n",
       " 'tangy spicy and sweet oh my',\n",
       " 'an indulgence with bite',\n",
       " 'the best have had',\n",
       " 'excellent chip',\n",
       " 'salt and vinegar chips',\n",
       " 'delicious',\n",
       " 'like them',\n",
       " 'love kettle chips',\n",
       " 'best unsalted chips',\n",
       " 'so delicious yet my companions wont touch them',\n",
       " 'love kettle chips but not this flavor',\n",
       " 'maybe the worst chips ever',\n",
       " 'surprise it is different',\n",
       " 'tasty',\n",
       " 'crisp',\n",
       " 'vinegar not my taste',\n",
       " 'spicy thai chips',\n",
       " 'delicious what else did you expect',\n",
       " 'great value',\n",
       " 'have had better jalapeno kettle chips',\n",
       " 'spicy but good',\n",
       " 'boulder salt and malt vinegar chips are way better',\n",
       " 'potato chips',\n",
       " 'lightly salted heavily delicious',\n",
       " 'too much flavor',\n",
       " 'love at first bite tongue puckering tang and crunch',\n",
       " 'the best chips period',\n",
       " 'delicious extra crunchy',\n",
       " 'chip snob alert',\n",
       " 'best salt vinegar',\n",
       " 'gourmet powerful salt vinegar chips',\n",
       " 'they changed the chips now they taste horrible',\n",
       " 'do not miss the salt',\n",
       " 'great deal',\n",
       " 'best chips out there',\n",
       " 'great price but not as tangy as expected',\n",
       " 'burnt',\n",
       " 'gaaak an extreme potato chip',\n",
       " 'kettle chips',\n",
       " 'absotively posilutely delicious',\n",
       " 'gone down hill',\n",
       " 'completely ripped off',\n",
       " '',\n",
       " 'not the best',\n",
       " 'stale beware buying these on special',\n",
       " 'these are very good',\n",
       " 'used to be my favorite chips',\n",
       " 'stale rancid oil taste and if you like even the tiniest bit of salt flavor on your chips',\n",
       " 'disgusting',\n",
       " 'taste terrible way too strong',\n",
       " 'over fried',\n",
       " 'no salt kettle chips',\n",
       " 'not as good as the english sell',\n",
       " 'very disappointed',\n",
       " 'dripping in oil',\n",
       " 'not so good',\n",
       " 'bags salt with chips added',\n",
       " 'chips',\n",
       " 'garbage',\n",
       " 'they are not madhouse munchies',\n",
       " 'too sour',\n",
       " 'awful taste',\n",
       " 'delicious',\n",
       " 'kettle has sold out the chips are horrible now',\n",
       " 'the bags were damaged with holes and stains',\n",
       " 'kettle chips make great mouse food',\n",
       " 'orgasmic',\n",
       " 'tang that packs punch',\n",
       " 'great new flavor',\n",
       " 'horrible cant believe this',\n",
       " 'favorite kettle flavor and great value',\n",
       " 'kettle brand potato chips new york cheddar',\n",
       " 'my favorite flavor',\n",
       " 'so much flavor your farts will smell like sweet onions',\n",
       " 'great chip',\n",
       " 'awesome and delicious',\n",
       " 'the only thing have ever been addicted too these chips',\n",
       " 'good chips',\n",
       " 'what happened the recipe has changed',\n",
       " 'yum if you want snack have something really good',\n",
       " 'good buy',\n",
       " 'yoli',\n",
       " 'good chips',\n",
       " 'awesome',\n",
       " 'burns the skin off your lips',\n",
       " 'great chips with very low sodium',\n",
       " 'kettle chips',\n",
       " 'spicy thai',\n",
       " 'smiles',\n",
       " 'my favorite kettle chip',\n",
       " 'best salt vinegar chips out there',\n",
       " 'amazing service',\n",
       " 'crunchy and spicy',\n",
       " 'pretty tasty and decently spiced',\n",
       " 'great tasting chips',\n",
       " 'kettle chips sea salt',\n",
       " 'good deal but close expiration date',\n",
       " 'great tasting chips',\n",
       " 'buy these eat these be happy',\n",
       " 'eating them for years',\n",
       " 'these chips will make you fat',\n",
       " 'like spice get these',\n",
       " 'fantastic',\n",
       " 'sweet salty tangy the way snack should be',\n",
       " 'one bite and you will become chippoisseur',\n",
       " 'some of the best chips anywhere',\n",
       " 'the defacto standard for salt and vinegar chips',\n",
       " 'yummy',\n",
       " 'delicious',\n",
       " 'lightly salted yet tasty',\n",
       " 'yummy',\n",
       " 'caution kettle chips are addictive',\n",
       " 'the chip with kiss of salt',\n",
       " 'crunch wow',\n",
       " 'great strong flavor',\n",
       " 'yummy for your tummy',\n",
       " 'best chips out there',\n",
       " 'these chips tasted good',\n",
       " 'awesome chips',\n",
       " 'dont know if',\n",
       " 'delicious',\n",
       " 'kettle foods spicy thai chips',\n",
       " 'expired stock',\n",
       " 'these potato chips are yummy',\n",
       " 'only awful because sometimes they are awful',\n",
       " 'stars for price and taste',\n",
       " 'crispy crunchy and robust',\n",
       " 'tangy and delicious snack',\n",
       " 'best chip',\n",
       " 'not low salt',\n",
       " 'best chips have ever tasted',\n",
       " 'tasty',\n",
       " 'love the smaller bags',\n",
       " 'things you need to know',\n",
       " 'made mistake',\n",
       " 'ok but miss vickie are better',\n",
       " 'do they have to bite back',\n",
       " 'these chips are awesome if not best but',\n",
       " 'kettle chips',\n",
       " 'great deal',\n",
       " 'what great tea at this price',\n",
       " 'delicious',\n",
       " 'great well balanced earl grey',\n",
       " 'best earl grey ever',\n",
       " 'favorite earl grey tea',\n",
       " 'delicious',\n",
       " 'the best',\n",
       " 'huge success',\n",
       " 'for your health',\n",
       " 'shrimp stir fry',\n",
       " 'do not taste from bottle mix with vanilla for true flavor',\n",
       " 'bavarian creme flavor oil',\n",
       " 'the oldest soft drink is still the best',\n",
       " 'made in michigan since',\n",
       " 'caramel flavor excellent for baking and toppings',\n",
       " 'these weigh oz not fluid ounces',\n",
       " 'great buy',\n",
       " 'excellent sweetner',\n",
       " 'agave syrup',\n",
       " 'sugar substitute',\n",
       " 'good but container could be better',\n",
       " 'great stuff',\n",
       " 'healthy sweetener',\n",
       " 'great way replacing the sugar',\n",
       " 'great substitute sweetener',\n",
       " 'my go to sweetner',\n",
       " 'the best',\n",
       " 'yummy',\n",
       " 'healthy stuff',\n",
       " 'agave nectar',\n",
       " 'sweet success',\n",
       " 'great product',\n",
       " 'best price on agave nectar that have found',\n",
       " 'how this could be good',\n",
       " 'best tea ever had',\n",
       " 'that is spicy',\n",
       " 'who needs salsa when chips taste this good',\n",
       " 'delicious',\n",
       " 'organic yummy chips what more can you ask for',\n",
       " 'very different flavor',\n",
       " 'these are the best widely available bbq chips',\n",
       " 'my favorite chips from kettle',\n",
       " 'amazing taste best chip ever',\n",
       " 'tasty',\n",
       " 'plocky sweet smokey chipotle whole grain tortill',\n",
       " 'yummy chips',\n",
       " 'tasty',\n",
       " 'so faboo dangerous for those prone to chips binges',\n",
       " 'the spice will grow',\n",
       " 'the best tortilla chips have ever eaten',\n",
       " 'such an excellent chip',\n",
       " 'wrong flavor got country bbq instead of chili chipotle',\n",
       " 'great',\n",
       " 'plocky rice and beans tortilla chips',\n",
       " 'huge fan of these chips',\n",
       " 'plocky tortilla chips tasty and healthy',\n",
       " 'plocky three grain tortilla chips',\n",
       " 'tasty but make sure you have gum',\n",
       " 'do not stop carrying these chips',\n",
       " 'om nom nom nom',\n",
       " 'delicious and healthy',\n",
       " 'unique schrumshist and tasty tortilla chips',\n",
       " 'music to my palate',\n",
       " 'yummy',\n",
       " 'kettle organic chipotle potato chips',\n",
       " 'best kept secret',\n",
       " 'delicious and additive',\n",
       " 'fanfreakintastic',\n",
       " 'smokin',\n",
       " 'really good chips',\n",
       " 'plocky tortilla chips red beans rice',\n",
       " 'simply the best',\n",
       " 'addicted',\n",
       " 'perfect tortilla chip goodness',\n",
       " 'these chips are addictive',\n",
       " 'best tortilla chips ever',\n",
       " 'meh',\n",
       " 'the organic label is misleading',\n",
       " 'kind of bland',\n",
       " 'delicious chips',\n",
       " 'broken chips but tasty',\n",
       " 'the best chips ever',\n",
       " 'excellent tortilla chips',\n",
       " 'black beans never tasted so good',\n",
       " 'not my favorite chip',\n",
       " 'very timely delivery',\n",
       " 'very tasty but beware',\n",
       " 'great candy',\n",
       " 'candy',\n",
       " 'cute cute cute',\n",
       " 'surprising find',\n",
       " 'unparalleled taste',\n",
       " 'amazing',\n",
       " 'bit on the stale side',\n",
       " 'delicious',\n",
       " 'disappointed',\n",
       " 'perhaps something was wrong with the batch',\n",
       " 'down the drain',\n",
       " 'big disappointment',\n",
       " 'worst frosting ever',\n",
       " 'worst frosting ever',\n",
       " 'cherrybrook kitchen vanilla frosting',\n",
       " 'do not buy this frosting',\n",
       " 'dum dums for all',\n",
       " 'lots of pops',\n",
       " 'when you have no fridge but want meat what do you do',\n",
       " 'anti oxidant smoothie',\n",
       " 'perversion of taste',\n",
       " 'annie homegrown organic whole wheat shells white cheddar macaroni cheese oz',\n",
       " 'the flavor of the gods',\n",
       " 'cuttiest gum of the century',\n",
       " 'city steam not much steam in this brew',\n",
       " 'good soy sauce but not special mainly good as unique gift',\n",
       " 'what surprise',\n",
       " 'gets my vote',\n",
       " 'and though kikkoman was good',\n",
       " 'better than anything in the supermarket',\n",
       " 'best soy sauce ever',\n",
       " 'great sauce',\n",
       " 'outstanding product',\n",
       " 'non gmo',\n",
       " 'amazingly true to flavors',\n",
       " 'it is laxative',\n",
       " 'good tasting cup joe',\n",
       " 'best of the tassimo',\n",
       " 'great coffee terrible price',\n",
       " 'one of the better discs',\n",
       " 'note rating both coffee and seller',\n",
       " 'kona for tassimo',\n",
       " 'best wafers',\n",
       " 'tao of tea don get it',\n",
       " 'perfect sampler of milka chocolate',\n",
       " 'dad really liked these',\n",
       " 'great candy my family loved it',\n",
       " 'yummy',\n",
       " 'horrible dont buy it',\n",
       " 'kleri tea works great',\n",
       " 'where are the cranberries',\n",
       " 'our ferret loves this',\n",
       " 'fantastic',\n",
       " 'great little treats',\n",
       " 'not edible',\n",
       " 'speedy shipping',\n",
       " 'my sons loved making this as easy enough to do by themselves with adult supervision any age can do this neat project',\n",
       " 'spongetastic',\n",
       " 'not as easy as it looks',\n",
       " 'not one but twice came broken',\n",
       " 'rudolph gingerbread house',\n",
       " 'fun to paint',\n",
       " 'disappointment',\n",
       " 'cute item but expired',\n",
       " 'if could give rating under one star',\n",
       " 'horrid',\n",
       " 'delicious',\n",
       " 'love this tea to each her own taste buds',\n",
       " 'there is none greater',\n",
       " 'caramel chocolate',\n",
       " 'chocolate heaven',\n",
       " 'heavenly action',\n",
       " 'great chocolate',\n",
       " 'great item',\n",
       " 'choclate',\n",
       " 'is this serious bucks',\n",
       " 'yum',\n",
       " 'overpriced even on gold box',\n",
       " 'even at seems little overpriced',\n",
       " 'quick arrival great basket',\n",
       " 'adorable basket makes nice presentation',\n",
       " 'specialty party item',\n",
       " 'delicious',\n",
       " 'chocolate how can you go wrong',\n",
       " 'chocolate liquor cups',\n",
       " 'scottie',\n",
       " 'has very good flavor',\n",
       " 'odd fake flavor not recommended',\n",
       " 'good sugarless gum with stronger gum for better bubble blowing than other sugarless gums',\n",
       " 'great gum',\n",
       " 'delicious imo and helps weight loss',\n",
       " 'royal canin cocker',\n",
       " 'perfect for cockers',\n",
       " 'my dog loves this food',\n",
       " 'great deal',\n",
       " 'great food',\n",
       " 'buster loves this dog food',\n",
       " 'happy dogs',\n",
       " 'fast and great service',\n",
       " 'good food',\n",
       " 'happy dog',\n",
       " 'great way to order',\n",
       " 'good quality delivered on time human edible',\n",
       " 'rave review for rolled oats',\n",
       " 'excellent',\n",
       " 'very different taste from the made in uk or made in india version',\n",
       " 'not like other countries cadbury',\n",
       " 'great deal on great tea',\n",
       " 'best breakfast tea ever',\n",
       " 'great organic tea at great price',\n",
       " 'mediocre but cheap',\n",
       " 'delicious and consistently good quality',\n",
       " 'nice flavorful tea for the purist black tea addict',\n",
       " 'the best tea',\n",
       " 'great tea',\n",
       " 'this is very fine tasting full lea great value',\n",
       " 'not creamy',\n",
       " 'this caramel is fantastic',\n",
       " 'great caramels',\n",
       " 'where has this candy been',\n",
       " 'not for traditional caramel lovers',\n",
       " 'soda',\n",
       " 'aluminum free',\n",
       " 'this baking soda is the bomb',\n",
       " 'interesting info about baking soda',\n",
       " 'excellent quality product fast delivery',\n",
       " 'good oatmeal am on my second bag',\n",
       " 'mm mm oats',\n",
       " 'loved the oats',\n",
       " 'funny taste',\n",
       " 'awsome',\n",
       " 'very happy with pocky sticks',\n",
       " 'these are great',\n",
       " 'love this snack',\n",
       " 'chocolate was all melt',\n",
       " 'this item is awesome',\n",
       " 'great tasting snack if you get the japanese pocky',\n",
       " 'great product',\n",
       " 'delicious',\n",
       " 'fun popping crispy sticks dipped in chocolate special treat for manga fans but find local or wait till fall',\n",
       " 'yummy',\n",
       " 'does not taste as it should',\n",
       " 'pocky',\n",
       " 'great buy',\n",
       " 'good stuff',\n",
       " 'pocky sticks together',\n",
       " 'really wanted to like these',\n",
       " 'good berry flavor excellent price',\n",
       " 'the chocolate ones are much better',\n",
       " 'something has changed',\n",
       " 'good berry flavor',\n",
       " 'great taste but size it smaller',\n",
       " 'cheaper ingredients lowered quality',\n",
       " 'not fan',\n",
       " 'perfect size',\n",
       " 'pretty good overall',\n",
       " 'my child loves them',\n",
       " 'small but good',\n",
       " 'you cannot go wrong',\n",
       " 'yummy but small',\n",
       " 'be very berry wary',\n",
       " 'highly recomended',\n",
       " 'very berry snack bars',\n",
       " 'great snack',\n",
       " 'great bars for gluten free diets',\n",
       " 'absolutely vile',\n",
       " 'fantastic coffee best have ever had',\n",
       " 'strictly the best',\n",
       " 'best coffee',\n",
       " 'beautiful presentation and pretty good tea too',\n",
       " 'break easily',\n",
       " 'wonderful tea',\n",
       " 'cannot take the smell',\n",
       " 'our twins love this one with the subscription the price is fair',\n",
       " 'dessert to them but is it nutritional enough',\n",
       " 'repeated deliveries of broken jars great product though',\n",
       " 'good flavor too runny though',\n",
       " 'protein and vitamin',\n",
       " 'baby lilly says thumbs up',\n",
       " 'earth best rice lentil dinner',\n",
       " 'plastic in the food',\n",
       " 'my baby favorite nd stage food so far and great for constipation',\n",
       " 'sons favorite dinner',\n",
       " 'no issues',\n",
       " 'too thin',\n",
       " 'great place to start',\n",
       " 'good for you but not best flavor',\n",
       " 'favorite',\n",
       " 'runny and odd tasting',\n",
       " 'too runny',\n",
       " 'great taste for picky baby but very thin compared to others',\n",
       " 'great stuff',\n",
       " 'son loves it',\n",
       " 'love it',\n",
       " 'found shredded plastic in the baby food',\n",
       " 'my son loves it',\n",
       " 'meh my daughter eats most everything else this is not her thing',\n",
       " 'not as yummy as earth best other flavors',\n",
       " 'son nd favorite dish',\n",
       " 'had to toss out of jars',\n",
       " 'one of our favorites',\n",
       " 'our baby likes it',\n",
       " 'our baby favorite dinner',\n",
       " 'my daughter favorite jarred food',\n",
       " 'yum',\n",
       " 'the only jarred baby food my son ate',\n",
       " 'little more watery than other nd foods for eb',\n",
       " 'calories of yumminess',\n",
       " 'organic and tasty',\n",
       " 'one of my son favorites',\n",
       " 'great',\n",
       " 'they make the best baby food',\n",
       " 'favorite',\n",
       " 'geat product',\n",
       " 'the garbanzo beans in it give horrible gas',\n",
       " 'baby did not like it',\n",
       " 'my baby favorite dinner',\n",
       " 'baby loves it but there is plastic in it',\n",
       " 'baby likes this one',\n",
       " 'organic and tasty',\n",
       " 'my baby liked it',\n",
       " 'moms beware of plastic in the food',\n",
       " 'allday energy',\n",
       " 'the best',\n",
       " 'tasty fruit',\n",
       " 'beautiful fresh and it came easrly yey',\n",
       " 'fresh fruit dark chocolate',\n",
       " 'sassafras tea bags',\n",
       " 'sassafrass tea',\n",
       " 'good tasten tea',\n",
       " 'hard to find tea',\n",
       " 'better taste than expected',\n",
       " 'lovely',\n",
       " '',\n",
       " 'great product',\n",
       " 'sass tea',\n",
       " 'the most awful taste',\n",
       " 'these are good but',\n",
       " 'great product',\n",
       " 'great paste but way overpriced like more',\n",
       " 'so easy to use',\n",
       " 'omg do not buy',\n",
       " 'wonderful gravy',\n",
       " 'yeeeee hawww',\n",
       " 'best white gravy',\n",
       " 'pioneer gravy is great',\n",
       " 'eco sugar',\n",
       " 'more expensive online',\n",
       " 'very convenient and far better than instant coffee',\n",
       " 'no pot fresh coffee',\n",
       " 'simple convenient',\n",
       " 'brews an excellent cup of coffee quickly and easily',\n",
       " 'very convenient',\n",
       " 'great taste',\n",
       " 'delish',\n",
       " 'new to rooibos tea',\n",
       " 'tasted better than loose leaf rooibos',\n",
       " 'best matcha quality and price',\n",
       " 'the best',\n",
       " 'moore marinade gluten low sodium and msg free',\n",
       " 'great tasting diet tea with all natural ingredients',\n",
       " 'awesome sauce',\n",
       " 'love this hot sauce',\n",
       " 'great hot sauce',\n",
       " 'love this stuff',\n",
       " 'best sauce around',\n",
       " 'tasty hot sauce',\n",
       " 'best all around hot sauce',\n",
       " 'best hot sauce around',\n",
       " 'hot flavorful',\n",
       " 'great hot sauce and people who run it',\n",
       " 'this sauce is the shiznit',\n",
       " 'not hot',\n",
       " 'not hot not habanero',\n",
       " 'best babka',\n",
       " 'my dog loves these but',\n",
       " 'she loves them',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1zLpnqsFxey"
   },
   "outputs": [],
   "source": [
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KT_D2cLiLy77"
   },
   "source": [
    "# Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393213, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vm8Fk2TCL7Sp"
   },
   "source": [
    "# Memahami distribusi kata dari urutan ulasan\n",
    "\n",
    "Di sini, kami akan menganalisis panjang ulasan dan ringkasan untuk mendapatkan ide keseluruhan tentang distribusi panjang teks. Ini akan membantu kami memperbaiki panjang maksimum urutan ulasan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3708,
     "status": "ok",
     "timestamp": 1587499504991,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "MdF76AHHFxgw",
    "outputId": "9a92033a-2f1f-479f-af7f-da542bb14058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000015DA3122908>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000015DA537A248>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# mengisi list dengan panjang kalimat\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwdSGIhGMEbz"
   },
   "source": [
    "Kami di sini dapat memperbaiki panjang maksimum ringkasan hingga 8 karena itu tampaknya menjadi panjang ringkasan mayoritas.\n",
    "\n",
    "kami mencoba memahami proporsi panjang ringkasan di bawah 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1780,
     "status": "ok",
     "timestamp": 1587499511276,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "7JRjwdIOFxg3",
    "outputId": "bfb06b64-2cda-4d8a-9bb8-b17a12639055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458130834941876\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split())<=8):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYB4Ga9KMjEu"
   },
   "source": [
    "Kami mengamati bahwa 94% dari ringkasan memiliki panjang di bawah 8. Jadi, kami dapat memperbaiki panjang maksimum ringkasan ke 8.\n",
    "\n",
    "Mari kita perbaiki panjang tinjauan maksimum menjadi 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKD5VOWqFxhC"
   },
   "outputs": [],
   "source": [
    "max_text_len=30\n",
    "max_summary_len=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d48E-8M4VO"
   },
   "source": [
    "Selanjutnya kami memilih ulasan dan ringkasan yang panjangnya di bawah atau sama dengan **max_text_len** dan **max_summary_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY0tEJP0FxhI"
   },
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tR1uh8xSNUma"
   },
   "source": [
    "Untuk menambahkan **START** dan **END** token khusus di awal dan akhir ringkasan. Di sini, kami telah memilih **sostok** dan **eostok** sebagai token START dan END\n",
    "\n",
    "Sebelumnya kami memastikan bahwa token khusus yang dipilih tidak pernah muncul dalam ringkasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwLUH78CFxhg"
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GlcX4RFOh13"
   },
   "source": [
    "Sebelum kami membangun model, kami perlu membagi dataset kami menjadi set train dan validation. Kami akan menggunakan porsi 90% dari dataset sebagai data train dan 10% yang tersisa sebagai data validation (set holdout):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RakakKHcFxhl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vq1mqyOHOtIl"
   },
   "source": [
    "# Mempersiapkan Tokenizer\n",
    "\n",
    "Tokenizer membangun kosakata dan mengubah urutan kata menjadi urutan bilangan bulat. Membuat tokenizer untuk teks dan ringkasan:\n",
    "\n",
    "# Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRHTgX6hFxhq"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#mempersiapkan tokenizer untuk ulasan pada data training\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzvLwYL_PDcx"
   },
   "source": [
    "# Kata - Kata Langka dan Cakupannya\n",
    "\n",
    "Melihat proporsi kata-kata langka dan cakupan totalnya di seluruh teks\n",
    "\n",
    "Di sini, kami mendefinisikan threshold menjadi 4 yang berarti kata yang hitungnya di bawah 4 dianggap sebagai kata yang langka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1587499803916,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "y8KronV2Fxhx",
    "outputId": "162233f1-377f-40d7-a60c-81f01ed980dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% kata-kata langka dalam kosa kata: 68.11028647495395\n",
      "Cakupan total kata-kata langka: 1.4450730144421176\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% kata-kata langka dalam kosa kata:\",(cnt/tot_cnt)*100)\n",
    "print(\"Cakupan total kata-kata langka:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "So-J-5kzQIeO"
   },
   "source": [
    "**Sebagai catatan kami**:\n",
    "\n",
    "\n",
    "* **tot_cnt** memberikan ukuran kosakata (yang berarti setiap kata unik dalam teks)\n",
    "Â \n",
    "* **cnt** memberi kami no. kata-kata langka yang jumlahnya di bawah ambang batas\n",
    "\n",
    "* **tot_cnt - cnt** memberi kami kata paling umum\n",
    "\n",
    "Selanjutnya kami mendefinisikan tokenizer dengan kata-kata paling umum untuk ulasan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2giEsF3Fxh3"
   },
   "outputs": [],
   "source": [
    "# menyiapkan tokenizer untuk ulasan tentang data pelatihan\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# ubah urutan teks ke dalam urutan integer\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# padding nol hingga panjang maksimum\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# ukuran kosakata (+1 untuk padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8800,
     "status": "ok",
     "timestamp": 1587499830462,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "DCbGMsm4FxiA",
    "outputId": "37561530-d7d5-4b54-af52-14099f0ab223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17489"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQfKP3sqRxi9"
   },
   "source": [
    "# Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRHqyBkBFxiJ"
   },
   "outputs": [],
   "source": [
    "# menyiapkan tokenizer untuk ulasan tentang data training\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KInA6O6ZSkJz"
   },
   "source": [
    "# Kata - Kata Langka dan Cakupannya\n",
    "\n",
    "Mari kita lihat proporsi kata-kata langka dan cakupan totalnya di seluruh ringkasan\n",
    "\n",
    "Di sini, kami mendefinisikan treshold menjadi 6 yang berarti kata yang hitungnya di bawah 6 dianggap sebagai kata yang langka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8147,
     "status": "ok",
     "timestamp": 1587499832931,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "yzE5OiRLFxiM",
    "outputId": "ba29fab3-a600-4927-bda7-cb2b66298b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% kata-kata langka dalam kosa kata: 75.99750324098527\n",
      "Cakupan total kata-kata langka: 2.592810080219758\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% kata-kata langka dalam kosa kata:\",(cnt/tot_cnt)*100)\n",
    "print(\"Cakupan total kata-kata langka:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PBhzKuRSw_9"
   },
   "source": [
    "Selanjutnya kami akan mendefinisikan tokenizer dengan kata paling umum untuk ringkasan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fswLvIgFxiR"
   },
   "outputs": [],
   "source": [
    "# menyiapkan tokenizer untuk ulasan tentang data pelatihan\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# ubah urutan teks ke dalam urutan integer\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# beri padding nol hingga panjang maksimum\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# ukuran kosakata\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqwDUT5oTFmn"
   },
   "source": [
    "Kemudian kami memeriksa apakah jumlah token awal kata sama dengan panjang data pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1634,
     "status": "ok",
     "timestamp": 1587499843122,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "pR8IX9FRFxiY",
    "outputId": "9316c7b2-5109-4d70-a94b-053e6980259f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192402, 192402)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVFhFVguTTtw"
   },
   "source": [
    "Di sini, saya menghapus baris yang hanya berisi token **START** dan **END**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZ-vW82sFxih"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cx5NISuMFxik"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOtlDcthFxip"
   },
   "source": [
    "# Membangun model\n",
    "\n",
    "Step paling penting sebelum melakukan training data adalah membangun model berikut kami meringkas sedikit tentang model yang kami gunakan.\n",
    "\n",
    "**Return Sequences = True**: Ketika parameter sequences return diatur ke True, LSTM menghasilkan status hidden dan status sel untuk setiap catatan waktu\n",
    "\n",
    "**Return State = True**: Ketika kondisi pengembalian = True, LSTM menghasilkan status hidden dan status sel dari catatan waktu terakhir saja\n",
    "\n",
    "**Initial State**: Ini digunakan untuk menginisialisasi status internal LSTM untuk catatan waktu pertama\n",
    "\n",
    "**Stacked LSTM**: Tumpukan LSTM memiliki beberapa lapisan LSTM yang saling bertumpuk.\n",
    "\n",
    "Ini mengarah pada representasi urutan yang lebih baik. Kami bisa bereksperimen dengan banyak lapisan LSTM yang ditumpuk satu sama lain.\n",
    "\n",
    "Di sini, kami membangun LSTM 3 susun untuk pembuat encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3556,
     "status": "ok",
     "timestamp": 1587500284287,
     "user": {
      "displayName": "Moch. Chamdani Mustaqim",
      "photoUrl": "https://lh3.googleusercontent.com/-33nbQYx-djs/AAAAAAAAAAI/AAAAAAAACx4/SN7K3gZtORs/s64/photo.jpg",
      "userId": "12782341176521372008"
     },
     "user_tz": -420
    },
    "id": "zXef38nBFxir",
    "outputId": "1874a25a-a984-4cef-f4a1-d17a6b3d142e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 100)      1748900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 30, 300), (N 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 30, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 30, 300), (N 721200      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 30, 300), (N 721200      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 30, 300), (N 721200      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    500000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 30, 300), (N 721200      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_7[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 5000)   3005000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,723,800\n",
      "Trainable params: 10,723,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output3, state_h3, state_c3= encoder_lstm3(encoder_output2)\n",
    "\n",
    "#encoder lstm 4\n",
    "encoder_lstm4=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output4, state_h4, state_c4= encoder_lstm4(encoder_output3)\n",
    "\n",
    "#encoder lstm 5\n",
    "encoder_lstm5=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output5, state_h5, state_c5= encoder_lstm5(encoder_output4)\n",
    "\n",
    "#encoder lstm 6\n",
    "encoder_lstm6=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output6, state_h6, state_c6= encoder_lstm6(encoder_output5)\n",
    "\n",
    "#encoder lstm 7\n",
    "encoder_lstm7=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm7(encoder_output6)\n",
    "\n",
    "# Membuat decoder, using `encoder_states` sebagai initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Menggabungkan attention input dan decoder output LSTM\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# mendefinisikan model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZVlfRuMUcoP"
   },
   "source": [
    "Kami di sini menggunakan `sparse_categorical_crossentropy` sebagai fungsi loss karena fungsi tersebut mengubah urutan integer ke vektor one-hot dengan cepat. Dan menurut literatur hal mengatasi dapat masalah memori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lwfi1Fm8Fxiz"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0ykDbxfUhyw"
   },
   "source": [
    "Konsep `EarlyStopping` kami digunakan untuk menghentikan training jaringan saraf yang kami buat pada waktu yang tepat dengan parameter metrik yang kami tentukan. Di sini kami menggunakan validation loss (val_loss) sebagai parameter `EarlyStopping` yang kami buat. Model kami akan menghentikan proses training setelah validation loss meningkat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-A3J92MUljB"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "# from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mw6CVECaUq5b"
   },
   "source": [
    "Step terakhir adalah training model pada ukuran batch 128 dan memvalidasinya pada set holdout (yang merupakan 10% dari dataset kami):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 189522 samples, validate on 21051 samples\n",
      "Epoch 1/50\n",
      "   128/189522 [..............................] - ETA: 5:47:13WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal:  Blas GEMM launch failed : a.shape=(128, 100), b.shape=(100, 300), m=128, n=300, k=100\n\t [[{{node model/lstm_7/while/body/_1884/MatMul_3}}]]\n\t [[Reshape_17/_68]]\n  (1) Internal:  Blas GEMM launch failed : a.shape=(128, 100), b.shape=(100, 300), m=128, n=300, k=100\n\t [[{{node model/lstm_7/while/body/_1884/MatMul_3}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_28169]\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c20498a9e480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                    y_val.reshape(y_val.shape[0],\n\u001b[0;32m     12\u001b[0m                                                  \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                                  1)[:,1:]))\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mstop_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal:  Blas GEMM launch failed : a.shape=(128, 100), b.shape=(100, 300), m=128, n=300, k=100\n\t [[{{node model/lstm_7/while/body/_1884/MatMul_3}}]]\n\t [[Reshape_17/_68]]\n  (1) Internal:  Blas GEMM launch failed : a.shape=(128, 100), b.shape=(100, 300), m=128, n=300, k=100\n\t [[{{node model/lstm_7/while/body/_1884/MatMul_3}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_28169]\n\nFunction call stack:\ndistributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "history=model.fit([x_tr,y_tr[:,:-1]], \n",
    "                  y_tr.reshape(y_tr.shape[0],\n",
    "                               y_tr.shape[1],\n",
    "                               1)[:,1:] ,\n",
    "                  epochs=50,\n",
    "                  callbacks=[es],\n",
    "                  # verbose=2,\n",
    "                  batch_size=128, \n",
    "                  validation_data=([x_val,y_val[:,:-1]], \n",
    "                                   y_val.reshape(y_val.shape[0],\n",
    "                                                 y_val.shape[1], \n",
    "                                                 1)[:,1:]))\n",
    "stop_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Runtime training model: %.2fs' % (stop_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ezKYOp2UxG5"
   },
   "source": [
    "# Memahami plot Diagnostik\n",
    "\n",
    "Sekarang, kami akan memplot beberapa plot diagnostik untuk memahami hasil pembelajaran model dari waktu ke waktu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDTNLAURFxjE"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSyx-HvpUz2o"
   },
   "source": [
    "Dari plot, kami dapat menyimpulkan bahwa val_loss telah meningkat setelah epoch 17 selama 2 epoch berturut-turut. Karenanya, pelatihan dihentikan pada epoch 19.\n",
    "\n",
    "Selanjutnya, kami membuat kamus untuk mengonversi indeks ke kata untuk kosakata target dan sumber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBX0zZnOFxjW"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM_nU_VvFxjq"
   },
   "source": [
    "Siapkan inferensi untuk pembuat enkode dan dekoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QkrNV-4Fxjt"
   },
   "outputs": [],
   "source": [
    "# Encode urutan input untuk mendapatkan vektor fitur\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Di bawah ini tensor akan menahan state langkah waktu sebelumnya\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Dapatkan embeddings dari urutan decoder\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "# Untuk memprediksi kata berikutnya dalam urutan, \n",
    "# atur status awal ke status dari langkah waktu sebelumnya\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# Lapisan softmax yang padat untuk menghasilkan masalah. atas kosakata target\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOiyk4ToWe74"
   },
   "source": [
    "Kami mendefinisikan fungsi di bawah ini yang merupakan implementasi dari proses inferensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6f6TTFnBFxj6"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Mengkodekan input sebagai vektor keadaan.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Hasilkan urutan target kosong yang panjangnya 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populasikan kata pertama dari urutan target dengan kata awal.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sampling token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Kondisi Exit: baik mencapai panjang max atau temukan kata berhenti.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Perbarui urutan target (dengan panjang 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Perbarui internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GuDf4TPWt6_"
   },
   "source": [
    "Mendefinisikan fungsi untuk mengubah urutan bilangan bulat menjadi urutan kata untuk ringkasan serta ulasan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAUntznIFxj9"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gM4ALyfWwA9"
   },
   "source": [
    "Berikut adalah beberapa ringkasan yang dihasilkan oleh model yang telah kami buat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUtQmQTmFxkI"
   },
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))    \n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTkaYNjHW4lC"
   },
   "source": [
    "# **Kesimpulan**\n",
    "\n",
    "Ini hal yang cukup bagus. Meskipun ringkasan aktual dan ringkasan yang dihasilkan oleh model kami tidak cocok dalam hal kata-kata, keduanya menyampaikan makna yang sama. Model kami mampu menghasilkan ringkasan yang dapat dibaca berdasarkan konteks yang ada dalam teks.\n",
    "\n",
    "Ini adalah bagaimana model kami dapat melakukan peringkasan teks menggunakan konsep deep learning dengan Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NLP_8LSTM_FullData.h5')\n",
    "model.save_weights('NLP_8LSTM_FullData_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP TUGAS TEKS PROCESSING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
